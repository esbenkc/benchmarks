{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks for safety using mechanistic interpretability\n",
    "\n",
    "This Jupyter notebook is set up to explore different ways we can use mechanistic interpretability to benchmark for safety in AI systems. A few preliminary research questions that I want to test out:\n",
    "- Can we use [DeepDecipher](https://github.com/apartresearch/deepdecipher) in a way to benchmark systems? Is there a possibility for an automated search query over many test tokens? Can we find pairs of baises? Can we find neurons associated with ill intent?\n",
    "- Can we memory edit models to perform better on [MACHIAVELLI](https://aypan17.github.io/machiavelli/)?\n",
    "- Can we operationalize different metrics on a per-neuron basis? Is there some sort of neuroscientific equivalent to cognitive dominance in neural networks like Transformers?\n",
    "- We have activation models with an explanability score. Can we generalize this in a useful fashion? Compare different explainability scores? Can we do a review of how neuroscience does this over correlation inside neural systems, e.g. with simpler models over clusters of neurons?\n",
    "- Are there other useful metrics per neuron than the ones mentioned in [DeepDecipher](https://github.com/apartresearch/deepdecipher) and can we use these to benchmark for safety?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
